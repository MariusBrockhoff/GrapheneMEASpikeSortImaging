{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jv7YN3CRzOkr"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "import keras.backend as K\n",
        "import os\n",
        "import sys\n",
        "import h5py\n",
        "import math\n",
        "import csv\n",
        "import pickle\n",
        "import glob\n",
        "import statistics\n",
        "!pip install detecta\n",
        "from detecta import detect_peaks\n",
        "\n",
        "\n",
        "\n",
        "from scipy.signal import butter, filtfilt, lfilter, ellip\n",
        "\n",
        "\n",
        "# MCS PyData tools\n",
        "!pip install McsPyDataTools\n",
        "import McsPy\n",
        "import McsPy.McsData\n",
        "from McsPy import ureg, Q_\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, RepeatVector, TimeDistributed, Dropout\n",
        "from keras.models import Model\n",
        "from keras import callbacks\n",
        "from keras.initializers import VarianceScaling\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, AffinityPropagation\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "from keras.constraints import UnitNorm, Constraint\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "nmi = normalized_mutual_info_score\n",
        "ari = adjusted_rand_score\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCZ_6c9Gpx6S"
      },
      "outputs": [],
      "source": [
        "def creating_y_labels_from_parameters_file(parameters, classification_task):\n",
        "    \"\"\"\n",
        "    Fucntion to extract labels from paramters file depending on the classfication task\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- parameters 2d array c\n",
        "    classification_task -- Desired classification task, values: ['PyramidalvsInter', 'ExcvsInh']\n",
        "\n",
        "    Returns:l\n",
        "    y -- array containing the labels for every training example, shape = (#training examples, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    if classification_task == 'PyramidalvsInter':\n",
        "        y = np.empty(shape=(parameters.shape[0],1))\n",
        "        for i in range(0, parameters.shape[0]):\n",
        "              if parameters[i][6] == 'p': #pyramidal neuron\n",
        "                y[i] = 0\n",
        "              elif parameters[i][6] == 'i': #Interneuron\n",
        "                y[i] = 1\n",
        "              else:\n",
        "                   y[i] = 2 #Neither pyramidal nor interneuron\n",
        "\n",
        "    if classification_task == 'ExcvsInh':\n",
        "        y = np.empty(shape=(parameters.shape[0],1))\n",
        "        for i in range(0, parameters.shape[0]):\n",
        "            if parameters[i][8] != 0 and parameters[i][9] == 0: #excitatory postsynaptic connection\n",
        "                   y[i] = 0\n",
        "            elif parameters[i][8] == 0 and parameters[i][9] != 0: #inhibitory postsynaptic connection\n",
        "                   y[i] = 1\n",
        "            elif parameters[i][8] != 0 and parameters[i][9] != 0: #both connection types\n",
        "                   y[i] = 3\n",
        "            else:\n",
        "                y[i] = 2 #Neither excitatory or inhibitory postsynaptic connection\n",
        "\n",
        "    return y\n",
        "\n",
        "def isolate_maximum_electrode(X, length=32):\n",
        "  X_maxelec = np.empty(shape=(X.shape[0], length))\n",
        "  for i in range(X.shape[0]):\n",
        "    maxind = np.argmax(X[i,:])\n",
        "    minind = np.argmin(X[i,:])\n",
        "    if X[i,maxind] >= np.absolute(X[i,minind]):\n",
        "      idx = maxind\n",
        "    else:\n",
        "      idx = minind\n",
        "    elec = int(idx / 32)\n",
        "    X_maxelec[i,:] = X[i,elec*32:(elec+1)*32]\n",
        "  return X_maxelec\n",
        "\n",
        "def normalization(Data_train, Data_dev, Data_test, epsilon = 0.001, alignment_training_examples = 'stacked_rows'):\n",
        "  \"\"\"\n",
        "  Normalizes every example in the training, dev and test set in a feature-wise manner\n",
        "\n",
        "  Arguments:\n",
        "  Data_train -- A 2d array containing all training examples (stack of 1d training traces)\n",
        "  Data_dev -- A 2d array containing all dev data examples (stack of 1d dev traces)\n",
        "  Data_test -- A 2d array containing all test data examples (stack of 1d test traces)\n",
        "  alignment_training_examples -- Describes in which dimensions the training examples are stacked\n",
        "                                 (default = 'stacked_rows' means each training example is a row vector\n",
        "                                 and they are stacked along the second axis,\n",
        "                                 i.e. data.shape = (#training examples, #features per training example))\n",
        "  \"\"\"\n",
        "  if alignment_training_examples == 'stacked_rows':\n",
        "      #substract mean\n",
        "      means = np.mean(Data_train, axis=0)\n",
        "      means = means.reshape((1, Data_train.shape[1]))\n",
        "      Normalized_data_train = Data_train - means\n",
        "\n",
        "      #Normalize over variance\n",
        "      element_wise_squares = np.multiply(Normalized_data_train,Normalized_data_train)\n",
        "      variances = np.mean(element_wise_squares, axis=0)\n",
        "      variances = variances.reshape((1, Data_train.shape[1]))\n",
        "      Normalized_data_train /= (variances + epsilon)\n",
        "\n",
        "      #Normalize data_test with same values\n",
        "      Normalized_Data_dev = Data_dev - means\n",
        "      Normalized_Data_dev /= (variances + epsilon)\n",
        "      Normalized_Data_test = Data_test - means\n",
        "      Normalized_Data_test /= (variances + epsilon)\n",
        "\n",
        "  elif alignment_training_examples == 'stacked_columns':\n",
        "      #substract mean\n",
        "      means = np.mean(Data_train, axis=1)\n",
        "      means = means.reshape((Data_train.shape[0], 1))\n",
        "      Normalized_data_train = Data_train - means\n",
        "\n",
        "      #Normalize over variance\n",
        "      element_wise_squares = np.multiply(Normalized_data_train,Normalized_data_train)\n",
        "      variances = np.mean(element_wise_squares, axis=1)\n",
        "      variances = variances.reshape((Data_train.shape[0], 1))\n",
        "      Normalized_data_train /= (variances + epsilon)\n",
        "\n",
        "      #Normalize data_dev and data_test with same values\n",
        "      Normalized_Data_dev = Data_dev - means\n",
        "      Normalized_Data_dev /= (variances + epsilon)\n",
        "      Normalized_Data_test = Data_test - means\n",
        "      Normalized_Data_test /= (variances + epsilon)\n",
        "\n",
        "  else:\n",
        "    raise ValueError(f'Please specify the stacking of your training examples in the argument alignment_training_examples to one of the valid options: stacked_rows, stacked_columns')\n",
        "\n",
        "  return Normalized_data_train, Normalized_Data_dev, Normalized_Data_test\n",
        "\n",
        "def normalization_train(Data_train, epsilon = 0.001, alignment_training_examples = 'stacked_rows'):\n",
        "  \"\"\"\n",
        "  Normalizes every training example of only the training set in a feature-wise manner\n",
        "\n",
        "  Arguments:\n",
        "  Data_train -- A 2d array containing all training examples (stack of 1d training traces)\n",
        "  Data_dev -- A 2d array containing all dev data examples (stack of 1d dev traces)\n",
        "  Data_test -- A 2d array containing all test data examples (stack of 1d test traces)\n",
        "  alignment_training_examples -- Describes in which dimensions the training examples are stacked\n",
        "                                 (default = 'stacked_rows' means each training example is a row vector\n",
        "                                 and they are stacked along the second axis,\n",
        "                                 i.e. data.shape = (#training examples, #features per training example))\n",
        "  \"\"\"\n",
        "  if alignment_training_examples == 'stacked_rows':\n",
        "      #substract mean\n",
        "      means = np.mean(Data_train, axis=0)\n",
        "      means = means.reshape((1, Data_train.shape[1]))\n",
        "      Normalized_data_train = Data_train - means\n",
        "\n",
        "      #Normalize over variance\n",
        "      element_wise_squares = np.multiply(Normalized_data_train,Normalized_data_train)\n",
        "      variances = np.mean(element_wise_squares, axis=0)\n",
        "      variances = variances.reshape((1, Data_train.shape[1]))\n",
        "      Normalized_data_train /= (variances + epsilon)\n",
        "\n",
        "  elif alignment_training_examples == 'stacked_columns':\n",
        "      #substract mean\n",
        "      means = np.mean(Data_train, axis=1)\n",
        "      means = means.reshape((Data_train.shape[0], 1))\n",
        "      Normalized_data_train = Data_train - means\n",
        "\n",
        "      #Normalize over variance\n",
        "      element_wise_squares = np.multiply(Normalized_data_train,Normalized_data_train)\n",
        "      variances = np.mean(element_wise_squares, axis=1)\n",
        "      variances = variances.reshape((Data_train.shape[0], 1))\n",
        "      Normalized_data_train /= (variances + epsilon)\n",
        "\n",
        "  else:\n",
        "    raise ValueError(f'Please specify the stacking of your training examples in the argument alignment_training_examples to one of the valid options: stacked_rows, stacked_columns')\n",
        "\n",
        "  return Normalized_data_train\n",
        "\n",
        "def Divide_train_dev_test(X, Y, fraction_list=[0.9, 0.05, 0.05], shuffle=False):\n",
        "  \"\"\"\n",
        "  Divides a dataset into training, development and test subset\n",
        "\n",
        "  Arguments:\n",
        "  X -- A 2d array containing dataset\n",
        "  Y -- A 2d array (shape = (#examples, 1) containing labels for each example)\n",
        "  fraction_list -- list containing size percentages each subset should contain, e.g. [0.9, 0.05, 0.05] for 90% train, 5% dev & 5% test\n",
        "  shuffle -- Boolean, if true do unison shuffling of X and Y\n",
        "\n",
        "  Returns:\n",
        "  X_train, Y_train, X_dev, Y_dev, X_test, Y_test -- Respective 2d array sub datasets\n",
        "  \"\"\"\n",
        "  if shuffle == True:\n",
        "    assert X.shape[0] == Y.shape[0]\n",
        "    p = np.random.permutation(X.shape[0])\n",
        "    X = X[p,:]\n",
        "    Y = Y[p,:]\n",
        "\n",
        "  X_train = X[0:round(fraction_list[0]*X.shape[0]),:]\n",
        "  Y_train = Y[0:round(fraction_list[0]*X.shape[0]),:]\n",
        "  X_dev = X[round(fraction_list[0]*X.shape[0]):round((fraction_list[0] + fraction_list[1])*X.shape[0]),:]\n",
        "  Y_dev = Y[round(fraction_list[0]*X.shape[0]):round((fraction_list[0] + fraction_list[1])*X.shape[0]),:]\n",
        "  X_test = X[round((fraction_list[0] + fraction_list[1])*X.shape[0]):,:]\n",
        "  Y_test = Y[round((fraction_list[0] + fraction_list[1])*X.shape[0]):,:]\n",
        "\n",
        "  return X_train, Y_train, X_dev, Y_dev, X_test, Y_test\n",
        "\n",
        "def acc(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate clustering accuracy. Require scikit-learn installed\n",
        "    # Arguments\n",
        "        y_true: true labels, numpy.array with shape `(n_samples,)`\n",
        "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
        "    # Return\n",
        "        accuracy, in [0,1]\n",
        "    \"\"\"\n",
        "    y_true = y_true.astype(np.int64)\n",
        "    y_pred = y_pred.astype(np.int64)\n",
        "    assert y_pred.size == y_true.size\n",
        "    D = max(y_pred.max(), y_true.max()) + 1\n",
        "    w = np.zeros((D, D), dtype=np.int64)\n",
        "    for i in range(y_pred.size):\n",
        "        w[y_pred[i], y_true[i]] += 1\n",
        "    from scipy.optimize import linear_sum_assignment\n",
        "    ind = linear_sum_assignment(w.max() - w)\n",
        "    ind = np.asarray(ind)\n",
        "    ind = np.transpose(ind)\n",
        "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size\n",
        "\n",
        "class ClusteringLayer(Layer):\n",
        "    \"\"\"\n",
        "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
        "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
        "\n",
        "    # Example\n",
        "    ```\n",
        "        model.add(ClusteringLayer(n_clusters=10))\n",
        "    ```\n",
        "    # Arguments\n",
        "        n_clusters: number of clusters.\n",
        "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
        "        alpha: degrees of freedom parameter in Student's t-distribution. Default to 1.0.\n",
        "    # Input shape\n",
        "        2D tensor with shape: `(n_samples, n_features)`.\n",
        "    # Output shape\n",
        "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
        "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
        "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
        "        super(ClusteringLayer, self).__init__(**kwargs)\n",
        "        self.n_clusters = n_clusters\n",
        "        self.alpha = alpha\n",
        "        self.initial_weights = weights\n",
        "        self.input_spec = layers.InputSpec(ndim=2)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 2\n",
        "        input_dim = input_shape[1]\n",
        "        self.input_spec = layers.InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
        "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
        "        if self.initial_weights is not None:\n",
        "            self.set_weights(self.initial_weights)\n",
        "            del self.initial_weights\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
        "         Measure the similarity between embedded point z_i and centroid µ_j.\n",
        "                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n",
        "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
        "                 (i.e., a soft assignment)\n",
        "        Arguments:\n",
        "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
        "        Return:\n",
        "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
        "        \"\"\"\n",
        "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
        "        q **= (self.alpha + 1.0) / 2.0\n",
        "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure each sample's 10 values add up to 1.\n",
        "        return q\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert input_shape and len(input_shape) == 2\n",
        "        return input_shape[0], self.n_clusters\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'n_clusters': self.n_clusters}\n",
        "        base_config = super(ClusteringLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "def target_distribution(q):\n",
        "  \"\"\"\n",
        "  computing an auxiliary target distribution\n",
        "  \"\"\"\n",
        "  weight = q ** 2 / q.sum(0)\n",
        "  return (weight.T / weight.sum(1)).T\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def gradient_transform(X, sr = 24000):\n",
        "  number_sampling_points = X.shape[1]\n",
        "  time_step = 1 / sr * 10**6 #in us\n",
        "  X_grad = np.empty(shape=(X.shape[0], X.shape[1]-1))\n",
        "  for i in range(X.shape[0]):\n",
        "    for j in range(X.shape[1]-1):\n",
        "      X_grad[i, j] = (X[i,j+1] - X[i,j])/time_step\n",
        "\n",
        "  return X_grad\n",
        "\n",
        "tableau20 = [(31, 119, 180), (255, 127, 14), (44, 160, 44), (214, 39, 40),(148, 103, 189),\n",
        "  (140, 86, 75), (227, 119, 194), (127, 127, 127), (188, 189, 34), (23, 190, 207)]\n",
        "\n",
        "  # Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.\n",
        "for i in range(len(tableau20)):\n",
        "  r, g, b = tableau20[i]\n",
        "  tableau20[i] = (r / 255., g / 255., b / 255.)\n",
        "\n",
        "\n",
        "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
        "        \"\"\"\n",
        "        Fully connected auto-encoder model, symmetric.\n",
        "        Arguments:\n",
        "            dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
        "                The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
        "            act: activation, not applied to Input, Hidden and Output layers\n",
        "        return:\n",
        "            (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
        "        \"\"\"\n",
        "        n_stacks = len(dims) - 1\n",
        "        # input\n",
        "        input_img = keras.Input(shape=(dims[0],), name='input')\n",
        "        x = input_img\n",
        "        # internal layers in encoder\n",
        "        for i in range(n_stacks-1):\n",
        "            x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n",
        "            #x = layers.Dropout(0.3)(x)\n",
        "\n",
        "\n",
        "        # hidden layer\n",
        "        encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)  # hidden layer, features are extracted from here\n",
        "\n",
        "        x = encoded\n",
        "        # internal layers in decoder\n",
        "        for i in range(n_stacks-1, 0, -1):\n",
        "            x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n",
        "            #x = layers.Dropout(0.3)(x)\n",
        "\n",
        "        # output\n",
        "        x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
        "        decoded = x\n",
        "        return keras.Model(inputs=input_img, outputs=decoded, name='AE'), keras.Model(inputs=input_img, outputs=encoded, name='encoder')\n",
        "\n",
        "\n",
        "#Functions\n",
        "def get_channel_data(data_stream, channel_ids = []):\n",
        "  \"\"\"s\n",
        "  Get the signal of the given channel over the course of time and in its measured range.\n",
        "  :param channel_id: ID of the channel\n",
        "  :return: Tuple (vector of the signal, unit of the values)\n",
        "  \"\"\"\n",
        "  if channel_ids == []:\n",
        "    signal = data_stream.channel_data[:, :]\n",
        "  else:\n",
        "    signal = data_stream.channel_data[data_stream.channel_infos[channel_ids].row_index, :]\n",
        "  scale = data_stream.channel_infos[0].adc_step.magnitude\n",
        "  #scale = self.channel_infos[channel_id].get_field('ConversionFactor') * (10**self.channel_infos[channel_id].get_field('Exponent'))\n",
        "  signal_corrected = (signal - data_stream.channel_infos[0].get_field('ADZero'))  * scale\n",
        "  return signal_corrected\n",
        "\n",
        "def plot_analog_stream_channel(analog_stream, channel_idx, from_in_s=0, to_in_s=None, show=True):\n",
        "    \"\"\"\n",
        "    Plots data from a single AnalogStream channel\n",
        "\n",
        "    :param analog_stream: A AnalogStream object\n",
        "    :param channel_idx: A scalar channel index (0 <= channel_idx < # channels in the AnalogStream)\n",
        "    :param from_in_s: The start timestamp of the plot (0 <= from_in_s < to_in_s). Default: 0\n",
        "    :param to_in_s: The end timestamp of the plot (from_in_s < to_in_s <= duration). Default: None (= recording duration)\n",
        "    :param show: If True (default), the plot is directly created. For further plotting, use show=False\n",
        "    \"\"\"\n",
        "    # extract basic information\n",
        "    ids = [c.channel_id for c in analog_stream.channel_infos.values()]\n",
        "    channel_id = ids[channel_idx]\n",
        "    channel_info = analog_stream.channel_infos[channel_id]\n",
        "    sampling_frequency = channel_info.sampling_frequency.magnitude\n",
        "\n",
        "    # get start and end index\n",
        "    from_idx = max(0, int(from_in_s * sampling_frequency))\n",
        "    if to_in_s is None:\n",
        "        to_idx = analog_stream.channel_data.shape[1]\n",
        "    else:\n",
        "        to_idx = min(analog_stream.channel_data.shape[1], int(to_in_s * sampling_frequency))\n",
        "\n",
        "    # get the timestamps for each sample\n",
        "    time = analog_stream.get_channel_sample_timestamps(channel_id, from_idx, to_idx)\n",
        "\n",
        "    # scale time to seconds:\n",
        "    scale_factor_for_second = Q_(1,time[1]).to(ureg.s).magnitude\n",
        "    time_in_sec = time[0] * scale_factor_for_second\n",
        "\n",
        "    # get the signal\n",
        "    signal = analog_stream.get_channel_in_range(channel_id, from_idx, to_idx)\n",
        "\n",
        "    # scale signal to µV:\n",
        "    scale_factor_for_uV = Q_(1,signal[1]).to(ureg.uV).magnitude\n",
        "    signal_in_uV = signal[0] * scale_factor_for_uV\n",
        "\n",
        "    # construct the plot\n",
        "    _ = plt.figure(figsize=(20,6))\n",
        "    _ = plt.plot(time_in_sec, signal_in_uV)\n",
        "    _ = plt.xlabel('Time (%s)' % ureg.s)\n",
        "    _ = plt.ylabel('Voltage (%s)' % ureg.uV)\n",
        "    _ = plt.title('Channel %s' % channel_info.info['Label'])\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "def butter_bandpass(lowcut, highcut, fs, order=2):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='bandpass')\n",
        "    return b, a\n",
        "\n",
        "def butter_bandpass_high(lowcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    b, a = butter(order, low, btype='highpass')\n",
        "    return b, a\n",
        "\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=2):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = lfilter(b, a, data,  axis=0) #filtfilt(b, a, data)\n",
        "    return y\n",
        "\n",
        "def butter_bandpass_filter_high(data, lowcut, fs, order=2):\n",
        "    b, a = butter_bandpass_high(lowcut, fs, order=order)\n",
        "    y = lfilter(b, a, data,  axis=0)\n",
        "    return y\n",
        "\n",
        "\n",
        "def ellip_filter(data, lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = ellip(order,0.1,40, [low, high], 'bandpass')\n",
        "    #y = filtfilt(b, a, data)\n",
        "    y = filtfilt(b, a, data,  axis=0, padtype = 'odd', padlen=3*(max(len(b),len(a))-1))\n",
        "    return y\n",
        "\n",
        "def ellip_filter_high(data, lowcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    b, a = ellip(order,0.1,40, low, 'highpass')\n",
        "    #y = filtfilt(b, a, data)\n",
        "    y = filtfilt(b, a, data,  axis=0, padtype = 'odd', padlen=3*(max(len(b),len(a))-1))\n",
        "    return y\n",
        "\n",
        "\n",
        "\n",
        "def spike_binning(X, time_window=0.1, fsample=20000, recording_len=None):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "  X: Standard array of shape Number of spikes x (Recording points per spike + 2) (e.g. 64 + 2; +2 for spike time and Electrode at which the spike was recorded)\n",
        "  time_window: Time window for binning, default: 0.1 s\n",
        "  fsample: sampling frequency\n",
        "  recording_len: Total lenght of recording (in s)\n",
        "\n",
        "  Outputs:\n",
        "  binned_spike_times: Array of shape Number of electrodes x (recording_len / time_window). For each electrode (rows), each column gives the number of spikes registered in the respective time interval\n",
        "  \"\"\"\n",
        "  #If no recording_len is explicitely give, take last recorded spike time and round up to generate an artificial recording length\n",
        "  if recording_len == None:\n",
        "    print(\"Make sure the sampling frequency is correct. Using fsample = \", fsample)\n",
        "    max_spike_time = np.max(X[:,1])\n",
        "    recording_len = np.ceil(max_spike_time/fsample)\n",
        "\n",
        "  #Get rid of spike shape info, not relevant here\n",
        "  X = X[:,:2]\n",
        "\n",
        "  #Define list of all recorded electrodes\n",
        "  electrode_list = np.unique(X[:,0])\n",
        "\n",
        "  #Calculate binned spike time representation\n",
        "  binned_spike_times = np.zeros(shape=(len(electrode_list), int(recording_len / time_window)))\n",
        "\n",
        "  for i in range(len(electrode_list)):\n",
        "    rel_spikes = X[X[:,0] == electrode_list[i], :]\n",
        "    for j in range(int(recording_len / time_window)):\n",
        "      timely_spikes = rel_spikes[rel_spikes[:,1] <= (j+1)*fsample,:]\n",
        "      timely_spikes = timely_spikes[timely_spikes[:,1] > j*fsample]\n",
        "      binned_spike_times[i,j] = timely_spikes.shape[0]\n",
        "\n",
        "  return binned_spike_times\n",
        "\n",
        "\n",
        "def binary_mutual_information(X, Y, quantile=75):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "  X,Y: binned spike time arrays of two electrodes of which we want to calculate the mutual information. I.e. two rows of spike_binning output\n",
        "  quantile: To simplify caclulations, we classify each time bin either as burst containing (set to 1) or non-member (set to 0). To decide if the spikes counted in a certain time interval should be considered\n",
        "  member or non-member, we apply the requirement that the number of spikes must the be in a certain quantile (default 75) of all bins of the respective electrode\n",
        "\n",
        "  Outputs:\n",
        "  MI: Mutual information score for the two electrodes\n",
        "  \"\"\"\n",
        "\n",
        "  TH_X = np.percentile(X, quantile)\n",
        "  TH_Y = np.percentile(Y, quantile)\n",
        "\n",
        "  X_bin = X > TH_X\n",
        "  Y_bin = Y > TH_Y\n",
        "\n",
        "  # get counts of each possible combination of\n",
        "  # classification at each time interval\n",
        "  a = len(Y_bin[X_bin==False][Y_bin[X_bin==False] == False])\n",
        "  b = len(Y_bin[X_bin==True][Y_bin[X_bin==True] == False])\n",
        "  c = len(Y_bin[X_bin==False][Y_bin[X_bin==False] == True])\n",
        "  d = len(Y_bin[X_bin==True][Y_bin[X_bin==True] == True])\n",
        "\n",
        "  # build dataframe of counts, apply artificial replacement\n",
        "  # of zeros with ones to prevent NaN being produced in computation\n",
        "  vals = np.array([[a,c],\n",
        "                  [b,d]])\n",
        "  vals[vals == 0] = 1\n",
        "\n",
        "\n",
        "  # compute mutual information by iterating over every possible\n",
        "  # combination of time interval classifs, get counts of intervals\n",
        "  n = np.sum(vals)\n",
        "  MI = 0\n",
        "\n",
        "  for i in range(vals.shape[0]):\n",
        "    for j in range(vals.shape[1]):\n",
        "      pi = np.sum(vals[i,:]) / n\n",
        "      pj = np.sum(vals[:,j]) / n\n",
        "      pij = vals[i,j] / n\n",
        "      MI = MI + (pij * math.log2(pij / (pi * pj)))\n",
        "\n",
        "  return MI\n",
        "\n",
        "def get_valid_electrode_pairs(X, max_dist=300, electrode_layout=\"MCS standard\"):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "  X: Standard array of shape Number of spikes x (Recording points per spike + 2) (e.g. 64 + 2; +2 for spike time and Electrode at which the spike was recorded)\n",
        "  max_dist: Maximum distance between electrodes to be considered a valid pair in um\n",
        "  electrode_layout: Spatial layout of electrodes. Currently, we only support the MultiChannelSystem 60 electrode standard layout\n",
        "\n",
        "\n",
        "  Output:\n",
        "  electrode_pairs: List of electrode pairs (tupels) that are valid\n",
        "  electrode_pairs_idx: List of corresponding electrode pair indices (tupels)\n",
        "  \"\"\"\n",
        "  if electrode_layout == \"MCS standard\":\n",
        "    inter_electrode_distance = 200\n",
        "    MCS_layout = np.array([[0,21,31,41,51,61,71,0],\n",
        "                      [12,22,32,42,52,62,72,82],\n",
        "                      [13,23,33,43,53,63,73,83],\n",
        "                      [14,24,34,44,54,64,74,84],\n",
        "                      [15,25,35,45,55,65,75,85],\n",
        "                      [16,26,36,46,56,66,76,86],\n",
        "                      [17,27,37,47,57,67,77,87],\n",
        "                      [0,28,38,48,58,68,78,0]])\n",
        "  else:\n",
        "    print(\"Error: Currently only MCS standard layout supported!\")\n",
        "\n",
        "  #Create list of all electrode pairs\n",
        "  electrodes = np.unique(X[:,0])\n",
        "  electrode_pairs = [(a, b) for idx, a in enumerate(electrodes) for b in electrodes[idx + 1:]]\n",
        "\n",
        "  #Filter out all pairs that have a larger eucl. distance than dist_max\n",
        "  delete_idx = list()\n",
        "  for pair in electrode_pairs:\n",
        "    pos1 = np.where(MCS_layout == pair[0])\n",
        "    pos2 = np.where(MCS_layout == pair[1])\n",
        "    x_dist = np.abs(pos1[0][0] - pos2[0][0])\n",
        "    y_dist = np.abs(pos1[1][0] - pos2[1][0])\n",
        "    euc_dist = np.sqrt((x_dist*inter_electrode_distance)**2 + (y_dist*inter_electrode_distance)**2)\n",
        "    if euc_dist > max_dist:\n",
        "      delete_idx.append(electrode_pairs.index(pair))\n",
        "\n",
        "  electrode_pairs = [i for j, i in enumerate(electrode_pairs) if j not in delete_idx]\n",
        "\n",
        "  #Convert to respective indices (which are 0-59 sorted)\n",
        "  electrode_pairs_idx = list()\n",
        "  for pair in electrode_pairs:\n",
        "    electrode_pairs_idx.append((electrodes.tolist().index(pair[0]), electrodes.tolist().index(pair[1])))\n",
        "\n",
        "  return electrode_pairs, electrode_pairs_idx\n",
        "\n",
        "\n",
        "def calculate_MI_pairs(X, time_window=0.1, fsample=20000, recording_len=None, max_dist=300, electrode_layout=\"MCS standard\", quantile=75):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "  X: Standard array of shape Number of spikes x (Recording points per spike + 2) (e.g. 64 + 2; +2 for spike time and Electrode at which the spike was recorded)\n",
        "  time_window: Time window for binning, default: 0.1 s\n",
        "  fsample: sampling frequency\n",
        "  recording_len: Total lenght of recording (in s)\n",
        "  max_dist: Maximum distance between electrodes to be considered a valid pair in um\n",
        "  electrode_layout: Spatial layout of electrodes. Currently, we only support the MultiChannelSystem 60 electrode standard layout\n",
        "  quantile: To simplify caclulations, we classify each time bin either as burst containing (set to 1) or non-member (set to 0). To decide if the spikes counted in a certain time interval should be considered\n",
        "  member or non-member, we apply the requirement that the number of spikes must the be in a certain quantile (default 75) of all bins of the respective electrode\n",
        "\n",
        "  Outputs:\n",
        "  MI_arr: Array of all Mutual information pairs of electrodes\n",
        "  \"\"\"\n",
        "  #Create binned array of spike times\n",
        "  binned_spike_times = spike_binning(X, time_window=time_window, fsample=fsample, recording_len=recording_len)\n",
        "\n",
        "  #Calculate valid electrode pairs\n",
        "  electrode_pairs, electrode_pairs_idx = get_valid_electrode_pairs(X, max_dist=max_dist, electrode_layout=electrode_layout)\n",
        "\n",
        "\n",
        "  #Calculate Mutual information for all pairs\n",
        "  MI_arr = np.zeros(len(electrode_pairs_idx))\n",
        "  for i in range(len(electrode_pairs_idx)):\n",
        "    MI_arr[i] = binary_mutual_information(binned_spike_times[electrode_pairs_idx[i][0],:], binned_spike_times[electrode_pairs_idx[i][1],:], quantile=quantile)\n",
        "\n",
        "  return electrode_pairs, MI_arr\n",
        "\n",
        "\n",
        "def do_MI_analysis_across_recordings(data_list = [], condition_names=[], analysis_name=None, time_window=0.1, fsample=20000, max_dist=300, electrode_layout=\"MCS standard\", quantile=75, plot=True, plot_violin=True, normalize=False):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "  data_list: List of dicts including standard arrays of shape Number of spikes x (Recording points per spike + 2) (e.g. 64 + 2; +2 for spike time and Electrode at which the spike was recorded) of each condition, recording_len and sampling frequency\n",
        "  condition_names: List of condition names, e.g. condition_names=[\"Day 0\", \"Day 1\",...]\n",
        "  plot: Should results be plotted\n",
        "  normalize: Should results be normalized to MI score of first condition (as usually baseline)\n",
        "  time_window: Time window for binning, default: 0.1 s\n",
        "  max_dist: Maximum distance between electrodes to be considered a valid pair in um\n",
        "  electrode_layout: Spatial layout of electrodes. Currently, we only support the MultiChannelSystem 60 electrode standard layout\n",
        "  quantile: To simplify caclulations, we classify each time bin either as burst containing (set to 1) or non-member (set to 0). To decide if the spikes counted in a certain time interval should be considered\n",
        "  member or non-member, we apply the requirement that the number of spikes must the be in a certain quantile (default 75) of all bins of the respective electrode\n",
        "\n",
        "  Outputs:\n",
        "  MI_arr_list: List of arrays of all Mutual information pairs of valid electrodes for each condition\n",
        "  optional: plotting\n",
        "  \"\"\"\n",
        "\n",
        "  MI_arr_list = list()\n",
        "\n",
        "  for dat in data_list:\n",
        "    electrode_pairs, MI_arr = calculate_MI_pairs(dat['Raw_spikes'], time_window=time_window, fsample=dat['Sampling rate'], recording_len=dat['Recording len'], max_dist=max_dist, electrode_layout=electrode_layout, quantile=quantile)\n",
        "    MI_arr_list.append(MI_arr)\n",
        "\n",
        "  if plot_violin:\n",
        "    #Violinplot!\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_axes([1,1,1,1])\n",
        "    parts = ax.violinplot(MI_arr_list, positions=range(1,len(MI_arr_list)+1), showmeans=True)\n",
        "\n",
        "    for pc in parts['bodies']:\n",
        "        pc.set_edgecolor('black')\n",
        "        pc.set_alpha(0.3)\n",
        "    ax.set_xticks(range(1,len(MI_arr_list)+1))\n",
        "    ax.set_xlim(range(1,len(MI_arr_list)+1)[0]-0.5,range(1,len(MI_arr_list)+1)[-1]+0.5)\n",
        "    ax.set_xticklabels(condition_names)\n",
        "    ax.set_xlabel('Conditions')\n",
        "    ax.set_ylabel('Mutual information in [bits]')\n",
        "    ax.set_title(\"Mutual Information Analysis: \" + analysis_name)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  MI_mean_arr = np.array([element.mean() for element in MI_arr_list])\n",
        "  MI_SE = np.array([statistics.stdev(element) / np.sqrt(len(element)) for element in MI_arr_list])\n",
        "\n",
        "  if plot:\n",
        "    tableau20 = [(31, 119, 180), (255, 127, 14), (44, 160, 44), (214, 39, 40),(148, 103, 189),\n",
        "                (140, 86, 75), (227, 119, 194), (127, 127, 127), (188, 189, 34), (23, 190, 207)]\n",
        "    # Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.\n",
        "    for i in range(len(tableau20)):\n",
        "      r, g, b = tableau20[i]\n",
        "      tableau20[i] = (r / 255., g / 255., b / 255.)\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_axes([1,1,1,1])\n",
        "    ax.errorbar(range(1, len(data_list)+1), MI_mean_arr, yerr=MI_SE, capsize=8, mfc=tableau20[0], mec=tableau20[0], ecolor=tableau20[0])\n",
        "    #ax.plot(range(1, len(X)+1), means_sttc)\n",
        "    ax.set_xticks(range(1,len(data_list)+1))\n",
        "    ax.set_xlim(range(1,len(data_list)+1)[0]-0.5,range(1,len(data_list)+1)[-1]+0.5)\n",
        "    ax.set_xticklabels(condition_names)\n",
        "    ax.set_xlabel('Conditions')\n",
        "    ax.set_ylabel('Mutual information in [bits]')\n",
        "    ax.set_title(\"Mutual Information Analysis: \" + analysis_name)\n",
        "    plt.grid(color=(0.9, 0.9, 0.9),linestyle=\"-\", linewidth=1)\n",
        "    plt.show()\n",
        "\n",
        "    if normalize:\n",
        "      fig = plt.figure()\n",
        "      ax = fig.add_axes([1,1,1,1])\n",
        "      ax.errorbar(range(1, len(data_list)+1), MI_mean_arr/MI_mean_arr[0]*100, yerr=MI_SE/MI_mean_arr[0]*100, capsize=8, mfc=tableau20[0], mec=tableau20[0], ecolor=tableau20[0])\n",
        "      #ax.plot(range(1, len(X)+1), means_sttc)\n",
        "      ax.set_xticks(range(1,len(data_list)+1))\n",
        "      ax.set_xlim(range(1,len(data_list)+1)[0]-0.5,range(1,len(data_list)+1)[-1]+0.5)\n",
        "      ax.set_xticklabels(condition_names)\n",
        "      ax.set_xlabel('Conditions')\n",
        "      ax.set_ylabel('Normed Mutual information in [%]')\n",
        "      ax.set_title(\"Relative change in Mutual Information Analysis: \" + analysis_name)\n",
        "      plt.grid(color=(0.9, 0.9, 0.9),linestyle=\"-\", linewidth=1)\n",
        "      plt.show()\n",
        "\n",
        "  return condition_names, MI_mean_arr, MI_SE, electrode_pairs, MI_arr_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Filtering**"
      ],
      "metadata": {
        "id": "4dP-MMcrhfiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load data\n",
        "file_name = 'name'\n",
        "path = 'path' #add path\n",
        "file_path = path + file_name + '.h5'\n",
        "os.chdir(path)\n",
        "\n",
        "big_file = False #In case file is too big for RAM, dividde in two parts. Set to true and load 30 channels each\n",
        "\n",
        "file = McsPy.McsData.RawData(file_path)\n",
        "electrode_stream = file.recordings[0].analog_streams[0]\n",
        "if including_stimulation:\n",
        "  event_stream = file.recordings[0].event_streams[0].event_entity[1]\n",
        "  all_events = event_stream.get_events()\n",
        "  all_event_timestamps = event_stream.get_event_timestamps()[0]\n",
        "\n",
        "# extract basic information\n",
        "ids = [c.channel_id for c in electrode_stream.channel_infos.values()]\n",
        "sr = int(electrode_stream.channel_infos[0].sampling_frequency.magnitude)\n",
        "\n",
        "#Get signal\n",
        "if big_file:\n",
        "  step_size = 10\n",
        "  min_step = 0\n",
        "  max_step = 60\n",
        "  for i in range(min_step, max_step, step_size):\n",
        "    #signal = get_channel_data(electrode_stream, channel_ids = [j for j in range(i,i+step_size)])\n",
        "    scale_factor_for_uV = Q_(1,'volt').to(ureg.uV).magnitude\n",
        "    if i == min_step:\n",
        "      recording_data = (get_channel_data(electrode_stream, channel_ids = [j for j in range(i,i+step_size)]) * scale_factor_for_uV).T\n",
        "    else:\n",
        "      recording_data = np.concatenate((recording_data, (get_channel_data(electrode_stream, channel_ids = [j for j in range(i,i+step_size)]) * scale_factor_for_uV).T), axis=1)\n",
        "    print(\"iteration\", i+step_size, \"completed\")\n",
        "    print(\"recording.shape:\", recording_data.shape)\n",
        "\n",
        "else:\n",
        "  signal = get_channel_data(electrode_stream, channel_ids = [])\n",
        "  scale_factor_for_uV = Q_(1,'volt').to(ureg.uV).magnitude\n",
        "  recording_data = (get_channel_data(electrode_stream, channel_ids = []) * scale_factor_for_uV).T\n",
        "\n",
        "\n",
        "#Filtering\n",
        "# Sample rate and desired cutoff frequencies (in Hz).\n",
        "lowcut = 300\n",
        "highcut = 3000\n",
        "fsample = sr\n",
        "\n",
        "filtered = np.empty(shape=(recording_data.shape[0], recording_data.shape[1]))\n",
        "for i in range(recording_data.shape[1]):\n",
        "  filtered[:,i] = butter_bandpass_filter(recording_data[:,i], lowcut, highcut, fsample, order=2)\n",
        "\n",
        "del recording_data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oI0LBX75hiNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Spike detection**"
      ],
      "metadata": {
        "id": "r_AOhIP7hilO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reject_ch = [None] #if there is no damaged channels\n",
        "\n",
        "length_of_chunck = 360 #Divide recoridng in chunks (in s) to make the thresholding more dynamic\n",
        "no_chunks = math.ceil(filtered.shape[0]/fsample/length_of_chunck) #finding the number of chunks. (e.g. is recoring is 60.5s, no_chunks = 61)#no_chunks = 20 #for specifying a time window manually\n",
        "print(\"Number of chunks:\", no_chunks)\n",
        "spk = list()\n",
        "spike_times = [[0] for x in range(filtered.shape[1])]\n",
        "ids = [c.channel_id for c in electrode_stream.channel_infos.values()]\n",
        "\n",
        "\n",
        "points_pre = 20\n",
        "points_post = 44\n",
        "\n",
        "overlap = 20 #10 data point overlap between chunks to check whetehr it is the local minima.\n",
        "t = 0 #rejects the 1st second to avoid ripples in filter\n",
        "\n",
        "while t < no_chunks: #rejects the incomplete chunk at the end to avoid filter ripple\n",
        "  if (t+1)*fsample*length_of_chunck <= len(filtered):\n",
        "    chunk = filtered[t*fsample*length_of_chunck:((t+1)*fsample*length_of_chunck + overlap - 1)]\n",
        "      #print(\"complete chunk\")\n",
        "  else:\n",
        "    chunk = filtered[t*fsample*length_of_chunck:]\n",
        "  #stdev = np.std(chunk, axis = 0)\n",
        "  med = np.median(np.absolute(chunk)/0.6745, axis=0)  #chunck\n",
        "\n",
        "  for index in range(len(chunk)):\n",
        "    if index > points_pre and index <fsample*length_of_chunck-points_post:\n",
        "      threshold_cross = chunk[index, :] < -5*med  #choose the threshold value. Finds which channels exceed the threshold at this instance\n",
        "      threshold_arti =  chunk[index, :] > -30*med\n",
        "\n",
        "      threshold = threshold_cross*threshold_arti\n",
        "\n",
        "\n",
        "      probable_spike = threshold #*stim_reject#finds out which electrode crosses -5*SD and ignores stimulation artefacts\n",
        "\n",
        "      if np.sum(probable_spike > 0):\n",
        "        for e in range(filtered.shape[1]):\n",
        "          if big_file==True:\n",
        "            channel_id = ids[e + min_step]\n",
        "            channel_info = electrode_stream.channel_infos[channel_id]\n",
        "            ch = int(channel_info.info['Label'][-2:])\n",
        "          else:\n",
        "            channel_id = ids[e]\n",
        "            channel_info = electrode_stream.channel_infos[channel_id]\n",
        "            ch = int(channel_info.info['Label'][-2:])\n",
        "          if probable_spike[e] == 1 and not(ch in reject_ch): #whether threshold exceeded at an electrode and if it is rejected\n",
        "            #bottom line does an additional check whether 3 points crosses -2.5*SD\n",
        "            #if probable_spike[e] and np.sum((filtered[(index-1):(index+2), e]) < -2.5*stdev[e]*np.ones(3)) == 3:\n",
        "            t_diff = (fsample*t*length_of_chunck + index) - spike_times[e][-1]\n",
        "            if t_diff > 0.002*fsample and chunk[index, e] == np.min(chunk[(index-points_pre):(index+points_post), e]): #and chunk[index, e] < -9: #whether the spike is 2ms apart and whether it is the true minumum and not just any point below -5*SD\n",
        "              spike_times[e].append(fsample*t*length_of_chunck + index)\n",
        "              if (fsample*t*length_of_chunck + index + points_post) < filtered.shape[0]: #making sure that the whole spike waveform is within the limits of the filtered signal array\n",
        "                spk_wave = list(filtered[(fsample*t*length_of_chunck + index - points_pre):(fsample*t*length_of_chunck + index + points_post), e])#selecting 1.6ms around the spike time from the whole fltered signal array\n",
        "                spk_wave.insert(0, (fsample*t*length_of_chunck + index))\n",
        "                spk_wave.insert(0, ch)\n",
        "                spk.append(spk_wave)\n",
        "\n",
        "\n",
        "  t = t+1\n",
        "  if t%5 == 0:\n",
        "    print(\"Completed chunk\", t)\n",
        "\n",
        "print(\"Total number of detected spikes:\", len(spk))\n",
        "f = open(path + '/Spikes_' + file_name + '.txt', 'a', newline='\\n')\n",
        "csv.writer(f, delimiter=\" \", ).writerows(spk)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "WNZS8sCRhlOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = path + '/Spikes_' + file_name + '.txt'\n",
        "X = np.genfromtxt(data_path, delimiter=' ')\n",
        "\n",
        "Results = {}\n",
        "Results[\"Filename\"] = file_name\n",
        "Results[\"Sampling rate\"] = fsample\n",
        "Results[\"Recording len\"] = filtered.shape[0] / fsample\n",
        "Results[\"Raw_spikes\"] = X[X[:, 1].argsort()]\n",
        "\n",
        "#save\n",
        "with open(path + '/Spike_File_' + file_name + '.pkl', 'wb+') as f:\n",
        "    pickle.dump(Results, f, -1)"
      ],
      "metadata": {
        "id": "YdPTRTMtkTjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz1GyFjbF76A"
      },
      "source": [
        "**3. Analyse Spike data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcvdiAOcEZ7P"
      },
      "outputs": [],
      "source": [
        "file_path = path + '/Spike_File_' + file_name + '.pkl'\n",
        "file_names = ['Day 1', 'Day 2', 'Day 3', 'Day 4']\n",
        "dict_list = list()\n",
        "recording_lens = []\n",
        "pseudo_cond_names = list([\"Day 1 / Baseline\", \"Day 2\", \"Day 3\", \"Day 4\"])\n",
        "analysis_name = \"Control_graphene\"\n",
        "sr = []\n",
        "#open files\n",
        "for i in range(len(file_names)):\n",
        "  with open(file_path + file_names[i] + '.pkl', 'rb') as f:\n",
        "    dict_list.append(pickle.load(f))\n",
        "    recording_lens.append(dict_list[i][\"Recording len\"])\n",
        "    sr.append(int(dict_list[i]['Sampling rate']))\n",
        "\n",
        "electrode_list = list()\n",
        "for p in range(len(dict_list)):\n",
        "  electrode_list = electrode_list + np.unique(dict_list[p]['Raw_spikes'][:,0]).tolist()\n",
        "electrode_list = list(np.unique(electrode_list))\n",
        "\n",
        "#Calculate firing overall rates\n",
        "res = np.zeros(shape=(len(electrode_list), 4))\n",
        "for el in electrode_list:\n",
        "  for j in range(4):\n",
        "    el_dict = dict_list[j]['Raw_spikes'][dict_list[j]['Raw_spikes'][:,0]==el,:]\n",
        "    res[electrode_list.index(el), j] = el_dict.shape[0]\n",
        "res = res / np.array(recording_lens)\n",
        "rates = np.zeros(shape=(1,4))\n",
        "for j in range(len(dict_list)):\n",
        "  rates[0,j] = dict_list[j]['Raw_spikes'].shape[0] / recording_lens[j]\n",
        "print(\"Firing rates:\", rates)\n",
        "rates = (rates / rates[0,0] - 1)*100\n",
        "print(\"Relative change in firing rate vs. Day 1:\", rates)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKYvbUR6iyuf"
      },
      "source": [
        "**Synchronicity analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr2CkTtui3I_"
      },
      "outputs": [],
      "source": [
        "dt = 0.1\n",
        "max_dist = 300\n",
        "\n",
        "condition_names, MI_mean_arr, MI_SE, electrode_pairs, MI_arr_list = do_MI_analysis_across_recordings(data_list = dict_list, condition_names=pseudo_cond_names, analysis_name=analysis_name, plot=True, normalize=False, time_window=dt, fsample=20000, max_dist=max_dist, electrode_layout=\"MCS standard\", quantile=75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcLAbw41Tb7A"
      },
      "source": [
        "**Spike Sort Filtering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8suytQqTbaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64ea42fb-3dab-416f-f398-7f32e6f0eb13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(331339, 64)\n"
          ]
        }
      ],
      "source": [
        "#Data pre-processing\n",
        "X = np.empty(shape=(1,67))\n",
        "for p in range(len(dict_list)):\n",
        "  X = np.concatenate((X, np.concatenate((p*np.ones(shape=(len(dict_list[p]['Raw_spikes']),1)),dict_list[p]['Raw_spikes'][:,:]),axis=1)), axis=0)\n",
        "X = X[1:,:]\n",
        "\n",
        "cond = X[:,0]\n",
        "elecs = X[:,1]\n",
        "spike_times = X[:,2]\n",
        "X = X[:,3:]\n",
        "\n",
        "#Pre-processing\n",
        "X_grad = gradient_transform(X)\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_grad)\n",
        "\n",
        "del X_grad\n",
        "\n",
        "#initialize training parameters\n",
        "file_name = \"test\"\n",
        "dims = [X_train.shape[-1], 500, 500, 2000, 10]\n",
        "#init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')\n",
        "pretrain_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "pretrain_epochs = 100\n",
        "batch_size = 4096\n",
        "save_dir = save_path\n",
        "name_save_process = \"test\" + '.h5'\n",
        "\n",
        "#initialize & train\n",
        "auto, encoder = autoencoder(dims)\n",
        "auto.compile(optimizer=pretrain_optimizer, loss='mse')\n",
        "cb = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", min_delta=0.0001, patience=10)\n",
        "history = auto.fit(X_train, X_train, batch_size=batch_size, epochs=pretrain_epochs, callbacks=cb)\n",
        "auto.save_weights(save_dir + name_save_process)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k-_0BcQ1PMr"
      },
      "outputs": [],
      "source": [
        "#Cluster in arbitrary number of classes for artefact detection\n",
        "n_clusters = 20\n",
        "kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
        "y_pred = kmeans.fit_predict(encoder.predict(X_train))\n",
        "print(np.unique(y_pred))\n",
        "for n in range(len(np.unique(y_pred))):\n",
        "  print(\"Cluster\", n, \":\", len(y_pred[y_pred==n]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_ODXSRfgNCx"
      },
      "outputs": [],
      "source": [
        "n_clusters = 20\n",
        "kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
        "y_pred = kmeans.fit_predict(encoder.predict(X_train))\n",
        "print(np.unique(y_pred))\n",
        "for n in range(len(np.unique(y_pred))):\n",
        "  print(\"Cluster\", n, \":\", len(y_pred[y_pred==n]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvQrPnT4UlAs"
      },
      "outputs": [],
      "source": [
        "#plot spike classes\n",
        "t = np.linspace(0, 3.2, 64)\n",
        "for j in range(len(np.unique(y_pred))):\n",
        "  temp_spikes = X[y_pred[:]==j,:]\n",
        "\n",
        "  plt.plot(t, np.mean(temp_spikes, axis=0), color=tableau20[j%10], label='Class %d' %(j))\n",
        "  plt.fill_between(t, np.mean(temp_spikes, axis=0) - np.std(temp_spikes, axis=0), np.mean(temp_spikes, axis=0) + np.std(temp_spikes, axis=0), alpha=0.1, edgecolor=tableau20[j%10], facecolor=tableau20[j%10])\n",
        "  plt.grid(color=(0.9, 0.9, 0.9),linestyle=\"-\", linewidth=1)\n",
        "  plt.xlim(0,3.2)\n",
        "  plt.ylim(-50,50)\n",
        "  plt.legend(loc=\"upper right\")\n",
        "  plt.ylabel(\"Voltage in [uV]\")\n",
        "  plt.xlabel(\"Time in [ms]\")\n",
        "  plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}